# -*- coding: utf-8 -*-
"""utils.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1__83dVJ25N4ycknJ6TctQ7SBJJ5m4_f1
"""

# pip install McsPyDataTools

# pip install tsaug


import os
# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
import sys
from fastdtw import fastdtw
from peak_finder import butter_bandpass_filter, get_peak_indices, find_poration_pulses
import pandas as pd
from scipy.signal import find_peaks,peak_widths
import os
import h5py as h5
import seaborn as sns
import glob
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.autograd import Variable
from torch.utils.data import Dataset, DataLoader
from scipy.signal import find_peaks,peak_widths
from sklearn.metrics import cohen_kappa_score,f1_score
from sklearn.model_selection import KFold, train_test_split
device = torch.device("cuda")
from sklearn.metrics import mean_squared_error
import tsaug
import pickle
import peak_finder
import matplotlib.pyplot as plt

from sklearn.metrics import cohen_kappa_score, f1_score
from sklearn.model_selection import KFold, train_test_split

from scipy.signal import find_peaks,peak_widths
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
import random

#Takes in an numpy array of intracellular traces and returns the decay rates in a numpy array (% decay per second)
def get_decay_rates(intras):
    decay_rates = []

    for intra in intras:
        intra = np.asarray(moving_filter(intra, 20))

        max_value = max(intra[:2800])
        max_index = np.where(intra == max_value)[0][0]

        min_value = min(intra[max_index:3750])
        min_index = np.where(intra[max_index:] == min_value)[0][0] + max_index

        time_diff = (max_index - min_index) * .0002

        decay_rate = (max_value - min_value) / (time_diff)
        decay_rate = decay_rate * -100
        decay_rates.append(decay_rate)

    decay_rates = np.asarray(decay_rates)
    return decay_rates

#Given numpy arrays of both the prediced traces and the actual traces, returns 
#numpy arrays of the 10 APDs taken from those traces
def get_predicted_apds(predicted_traces, actual_traces):  
    pred_widths_test = []
    raw_widths_test = []
    for pred_test,raw_test in zip(predicted_traces,actual_traces):
        w1_test, h1_test, l1_test = get_apds(pred_test, 2800)
        pred_widths_test.append(w1_test)
        w2_test, h2_test, l2_test = get_apds(raw_test, 2800)
        raw_widths_test.append(w2_test)
    pred_widths = np.asarray(pred_widths_test)
    raw_widths = np.asarray(raw_widths_test)
    return pred_widths, raw_widths

#Takes in numpy arrays of the apd predictions and true apd values and returns
#the indices sorted by mse, and returns the mse values for the predicted values
from sklearn.metrics import mean_squared_error

def sort_by_error(predicted_apds, raw_apds):
  mse_apd = []
  for pred_val,raw_val in zip(predicted_apds, raw_apds):
    mse = mean_squared_error(raw_val, pred_val)
    mse_apd.append(mse)
  error_sort = np.argsort(mse_apd)
  return error_sort, mse_apd

#Takes in a 1 dimensional numpy array of a single trace (4000 points) and adds
#some random noise
def add_noise(intra_trace):
  intra_trace = intra_trace.reshape(sig_len_extra)
  std = np.std(intra_trace[0:500])
  noise = np.random.normal(0,std/2,len(intra_trace))
  noisy_trace = intra_trace + noise
  return noisy_trace

#Takes in a 1 dimensional numpy array and a window size, and applies a moving
#average filter to the data.
def moving_filter(data, window_size):
  i = 0
  moving_averages = []
  while i < len(data) - window_size + 1:
      this_window = data[i : i + window_size]

      window_average = sum(this_window) / window_size
      moving_averages.append(window_average)
      i += 1

  for i in range(window_size - 1):
      moving_averages.append(data[len(data) - (window_size - i)])

  return moving_averages

#Takes in a numpy array representing a single intracellular trace (4000 points), and
#two_peak_cutoff, which is a point after the action potential but before a potential second
#peak. Returns the apd widths, heights (y values of where they occur in action potential), and
#left values (x values of where apd starts in action potential)
def get_apds(intra_trace, two_peak_cutoff):
  intra_trace = np.asarray(moving_filter(intra_trace, 20))
  period = pd.Series(intra_trace)
  std = period.rolling(window=20).std()
  stdofstd = np.std(std[19:])
  stdMedian = np.median(std[19:])

  start_loc = np.where(std == np.max(std[19:1500]))[0][0]

  while (start_loc >= 1 and std[start_loc] > stdMedian + 0.2 * stdofstd):
    start_loc = start_loc - 1;

  w1 = []
  h1 = []
  l1 = []

  period = period[start_loc:]
  two_peak_cutoff = two_peak_cutoff - start_loc #try +

  locs = np.where(intra_trace[two_peak_cutoff:] > 0.85)[0] + two_peak_cutoff
  if len(locs) == 0:
  #for relheight in np.linspace(0.05,0.9,15):
    for relheight in np.linspace(0.1,1,10):
      w,h,l,r=peak_widths(period,find_peaks(period,distance=3500)[0],rel_height=relheight)
      w1.append(w[0])
      h1.append(h[0])
      l1.append(l[0] + start_loc)
      
  else:
    distance = locs[0] - start_loc - 500
    if distance < 1:
        print(distance)
        distance = 1
    for relheight in np.linspace(0.1,1,10):
      w,h,l,r=peak_widths(period,find_peaks(period[0:locs[0]-200], distance=distance)[0],rel_height=relheight)
      w1.append(w[0])
      h1.append(h[0])
      l1.append(l[0] + start_loc)
  return np.asarray(w1), np.asarray(h1), np.asarray(l1)

#Given numpy arrays of predicted apds and correct_apds, returns arrays containing
#the percent error and absolute error of each apd value

def get_apd_errors(predicted_apds, correct_apds):
    apds_percent_errors = np.zeros((10, len(predicted_apds)))
    apds_absolute_errors = np.zeros((10, len(predicted_apds)))
    for i in range(len(predicted_apds)):
        for apd in range(10):
            correct = correct_apds[i][apd]
            predicted = predicted_apds[i][apd]

            percent_error = abs(((predicted - correct) / correct) * 100)
            absolute_error = abs(predicted - correct) * .0002
            apds_percent_errors[apd][i] = abs(percent_error)
            apds_absolute_errors[apd][i] = abs(absolute_error)
    return apds_percent_errors, apds_absolute_errors

#Given a numpy array of data, a low percentile value, and a high percentile value,
# returns the data within the two given percentiles 
def get_percentile_data(data, low_percentile, high_percentile):
  cutoff_low = np.percentile(data, low_percentile)
  cutoff_high = np.percentile(data, high_percentile)
  within_range = data < cutoff_high
  new_data = data[within_range]
  within_range = new_data > cutoff_low
  new_data = new_data[within_range]
  return new_data

#Applies some random jitter to an array (used for violin plots)
def rand_jitter(arr):
    stdev = .05 * (np.mean(arr))
    return arr + np.random.randn(len(arr)) * stdev

#Given numpy arrays of the extra and intra testing data, and apd values, and a model, returns the
#predicted traces (intras), actual traces, and predicted apds
def test_model(extracellular_testing, intracellular_testing, apds_testing, model):
  sig_len_extra = len(extracellular_testing[0])
  sig_len_apd = len(apds_testing[0])

  extras_testing=extracellular_testing.reshape(-1,sig_len_extra,1) #needs to be a tensor for pytorch, so add a dimension to Nxlen_sigx1
  intras_testing=intracellular_testing.reshape(-1,sig_len_extra,1)
  apds_testing=apds_testing.reshape(-1,sig_len_apd, 1) #overall scale

  extras_noisy = []

  for extra in extras_testing:
    extra_noisy = add_noise(extra)
    extras_noisy.append(extra_noisy)

  extras_testing = np.asarray(extras_noisy)
  extras_testing=extras_testing.reshape(-1,sig_len_extra,1)

  batch_size = 4
  valid = ION_Dataset(extras_testing, apds_testing, intras_testing, mode='valid')

  x_test = torch.tensor(np.transpose(extras_testing,(0,2,1)), dtype=torch.float).cuda()
  test = torch.utils.data.TensorDataset(x_test)

  test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)

  model.eval()
  test_preds_apd=[]
  test_preds_trace=[]
  test_input=[]
  for i, x_batch in enumerate(test_loader):
      y_pred_apd = model(x_batch[0])[0].detach()
      y_pred_trace = model(x_batch[0])[1].detach()
      test_preds_apd.append(np.asarray(y_pred_apd.cpu().detach())[:,0,:])
      test_preds_trace.append(np.asarray(y_pred_trace.cpu().detach())[:,0,:])
      test_input.append(np.asarray(x_batch[0].cpu().detach())[:,0,:])

  test_preds_apd=np.concatenate(test_preds_apd)
  test_preds_apd = test_preds_apd * 2320.1992383    #scaling is constant, will not change
  test_preds_trace=np.concatenate(test_preds_trace)
  test_input=np.concatenate(test_input)

  intras_testing=intras_testing.reshape(-1,sig_len_extra)
  intras_testing = np.asarray(intras_testing)

  return test_preds_trace, intras_testing, test_preds_apd

def top_5_apds(apd_abs_all,list1,save_as, abs_ = True):
    
        apd_abs_all['phy']= apd_abs_all['physics'].apply(lambda x: 1 if x== True or x =='True' else 0)
        apd_abs_all['mae']= apd_abs_all['loss_'].apply(lambda x: 1 if x== 'mae'  else 0)
        apds = apd_abs_all.groupby('Model').mean()
        if abs_: 
            apds['avg'] = apds[['apd_abs_' + str(i) for i in range(10)]].mean(axis = 1)
            apds['std'] = apds[['apd_abs_' + str(i) for i in range(10)]].std(axis = 1)
        else: 
            apds['avg'] = apds[['apd_perc_' + str(i) for i in range(10)]].mean(axis = 1)
            apds['std'] = apds[['apd_perc_' + str(i) for i in range(10)]].std(axis = 1)
        apds = apds.reset_index()
       
        apds['loss'] = apds['mae'].apply(lambda x: 'mae' if x == 1 else 'mse')
        cols_to_name = ['Model','mae','kernel_size', 'ch_num', 'depth',  'phy','avg','std','w1','w2','w3']
        
        for j in apds.index:
            name= ''.join([i +'_'+  str(round(apds.loc[j,i],4)) + ' | ' for i in cols_to_name ])
            apds.loc[j,'name'] = name

        
        models_dic = {}
        names_ = []
        
        if abs_ :
            for j in list1:
                # print(j)
                models_dic[j] = np.array(apd_abs_all[apd_abs_all['Model'] ==j ][['apd_abs_' + str(i) for i in range(10)]]).T
                names_.append(apds.loc[j,'name'])
            
            boxplot_comparison2(list( models_dic.values()),
                                 names_, 'Absolute Error (s)')
            plt.savefig(save_as+'box_abs.png')
        else :
            for j in list1:

                models_dic[j] = np.array(apd_abs_all[apd_abs_all['Model'] ==j ][['apd_perc_' + str(i) for i in range(10)]]).T
                names_.append(apds.loc[j,'name'])
            
            boxplot_comparison2(list( models_dic.values()),
                                 names_, '% Error')
            plt.savefig(save_as+'box_perc.png')

def get_predicted_apds(predicted_traces, actual_traces):  
    pred_widths_test = []
    raw_widths_test = []
    for pred_test,raw_test in zip(predicted_traces,actual_traces):
        w1_test, h1_test, l1_test = get_apds(pred_test, 4800)
        pred_widths_test.append(w1_test)
        w2_test, h2_test, l2_test = get_apds(raw_test, 4800)
        raw_widths_test.append(w2_test)
    pred_widths = np.asarray(pred_widths_test)
    raw_widths = np.asarray(raw_widths_test)
    return pred_widths, raw_widths

def calculate_accuracy(intra_predictions, intra_correct):
    pred_apds, correct_apds = get_predicted_apds(intra_predictions[:,:,0], intra_correct[:,:])
    apds_percent_errors, apds_absolute_errors = get_apd_errors(pred_apds, correct_apds)

    new_length = len(get_percentile_data(apds_percent_errors[0], 10, 90))
    apds_percent_errors_cutoff = np.zeros([10, new_length])
    apds_absolute_errors_cutoff = np.zeros([10, new_length])

    for apd in range(10):
        apds_percent_errors_cutoff[apd] = get_percentile_data(apds_percent_errors[apd], 10, 90)
        apds_absolute_errors_cutoff[apd] = get_percentile_data(apds_absolute_errors[apd], 10, 90)

    # return pred_apds, correct_apds, apds_percent_errors_cutoff, apds_absolute_errors_cutoff
    return pred_apds, correct_apds, apds_percent_errors, apds_absolute_errors


#Takes in a numpy array representing a single intracellular trace (4000 points), and
#two_peak_cutoff, which is a point after the action potential but before a potential second
#peak. Returns the apd widths, heights (y values of where they occur in action potential), and
#left values (x values of where apd starts in action potential)
def get_apds(intra_trace, two_peak_cutoff):
  intra_trace = np.asarray(moving_filter(intra_trace, 20))
  period = pd.Series(intra_trace)
  std = period.rolling(window=20).std()
  stdofstd = np.std(std[19:])
  stdMedian = np.median(std[19:])

  start_loc = np.where(std == np.max(std[19:1500]))[0][0]

  while (start_loc >= 1 and std[start_loc] > stdMedian + 0.2 * stdofstd):
    start_loc = start_loc - 1;

  w1 = []
  h1 = []
  l1 = []

  period = period[start_loc:]
  two_peak_cutoff = two_peak_cutoff - start_loc #try +

  locs = np.where(intra_trace[two_peak_cutoff:] > 0.85)[0] + two_peak_cutoff
  if len(locs) == 0:
  #for relheight in np.linspace(0.05,0.9,15):
    for relheight in np.linspace(0.1,1,10):
      w,h,l,r=peak_widths(period,find_peaks(period,distance=3500)[0],rel_height=relheight)
      w1.append(w[0])
      h1.append(h[0])
      l1.append(l[0] + start_loc)
      
  else:
    distance = locs[0] - start_loc - 500
    if distance < 1:
        print(distance)
        distance = 1
    for relheight in np.linspace(0.1,1,10):
      w,h,l,r=peak_widths(period,find_peaks(period[0:locs[0]-200], distance=distance)[0],rel_height=relheight)
      w1.append(w[0])
      h1.append(h[0])
      l1.append(l[0] + start_loc)
  return np.asarray(w1), np.asarray(h1), np.asarray(l1)

#Given numpy arrays of predicted apds and correct_apds, returns arrays containing
#the percent error and absolute error of each apd value

#Given a numpy array of data, a low percentile value, and a high percentile value,
# returns the data within the two given percentiles 
def get_percentile_data(data, low_percentile, high_percentile):
  cutoff_low = np.percentile(data, low_percentile)
  cutoff_high = np.percentile(data, high_percentile)
  within_range = data < cutoff_high
  new_data = data[within_range]
  within_range = new_data > cutoff_low
  new_data = new_data[within_range]
  return new_data

def boxplot_accuracy(apd_errors, color, positions):
  plt.boxplot(np.transpose(apd_errors),patch_artist=True,
            boxprops=dict(facecolor=color, color='k'),
            capprops=dict(color='k'),
            whiskerprops=dict(color=color),
            flierprops=dict(color=color, markeredgecolor=color),
            medianprops=dict(color='k'), widths = 0.1, positions = positions)

  plt.xticks([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], ['APD10', 'APD20', 'APD30', 'APD40', 'APD50', 'APD60', 'APD70', 'APD80', 'APD90', 'APD100'], rotation=90)

              
def boxplot_accuracy(apd_errors, color, positions):
  plt.boxplot(np.transpose(apd_errors),patch_artist=True,
            boxprops=dict(facecolor=color, color='k'),
            capprops=dict(color='k'),
            whiskerprops=dict(color=color),
            flierprops=dict(color=color, markeredgecolor=color),
            medianprops=dict(color='k'), widths = 0.1, positions = positions)

  plt.xticks([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], ['APD10', 'APD20', 'APD30', 'APD40', 'APD50', 'APD60', 'APD70', 'APD80', 'APD90', 'APD100'], rotation=90)

#Given the errors from 2 models, plots them together to compare the percent/absolute
#errors.
def boxplot_comparison(errors_model_1, errors_model_2, ylabel):
  plt.rcParams["figure.figsize"] = (12,4)
  boxplot_accuracy(errors_model_1, 'b', [0.9, 1.9, 2.9, 3.9, 4.9, 5.9, 6.9, 7.9, 8.9, 9.9])
  boxplot_accuracy(errors_model_2, 'r', [1.1, 2.1, 3.1, 4.1, 5.1, 6.1, 7.1, 8.1, 9.1, 10.1])
  plt.ylabel(ylabel)

def boxplot_comparison2(errors_model_1, errors_model_2,errors_model_3, ylabel):
  plt.rcParams["figure.figsize"] = (12,4)
  boxplot_accuracy(errors_model_1, 'b', [0.9, 1.9, 2.9, 3.9, 4.9, 5.9, 6.9, 7.9, 8.9, 9.9])
  boxplot_accuracy(errors_model_2, 'r', [1.1, 2.1, 3.1, 4.1, 5.1, 6.1, 7.1, 8.1, 9.1, 10.1])
  boxplot_accuracy(errors_model_3, 'g', [1.3, 2.3, 3.3, 4.3, 5.3, 6.3, 7.3, 8.3, 9.3, 10.3])
  plt.ylabel(ylabel)

#Given the intracellular traces, predicted traces, extracellular traces, and the sorted indices,
#plots the 5 best predictions from the dataset along with their extracellular counterparts.
def plot_best_predictions(intras_test, pred_intras_test, extras_test, test_error_sort):
  f,ax=plt.subplots(2,5,figsize=(16,3))

  color = (1.00,0.41,0.16)
  for ranking in range(5):
    idx = test_error_sort[ranking]
    ax[0, ranking].plot(intras_test[idx], color='b', linewidth = 4)
    ax[0, ranking].plot(pred_intras_test[idx], color=color)
    #plt.legend(['Actual', 'Predicted'])

    ax[1, ranking].plot(np.asarray(extras_test)[idx], color='b', linewidth = 4)
    ax[1, ranking].set_ylim(-0.5, 0.5)
#Given the intracellular traces, predicted traces, extracellular traces, and the sorted indices,
#plots the 5 worst predictions from the dataset along with their extracellular counterparts.
def plot_worst_predictions(intras_test, pred_intras_test, extras_test, test_error_sort):
  f,ax=plt.subplots(2,5,figsize=(16,3))

  color = (1.00,0.41,0.16)
  for i in range(5):
    idx = test_error_sort[len(intras_test) - i - 1]
    ax[0, i].plot(intras_test[idx], color='b', linewidth = 4)
    ax[0, i].plot(pred_intras_test[idx], color=color)
    #plt.legend(['Actual', 'Predicted'])

    ax[1, i].plot(np.asarray(extras_test)[idx], color='b', linewidth = 4)
    ax[1, i].set_ylim(-0.5, 0.5)
    
def calculate_accuracy(intra_predictions, intra_correct):
  pred_apds, correct_apds = get_predicted_apds(intra_predictions[:,:,0], intra_correct[:,:])
  apds_percent_errors, apds_absolute_errors = get_apd_errors(pred_apds, correct_apds)

  new_length = len(get_percentile_data(apds_percent_errors[0], 10, 90))
  apds_percent_errors_cutoff = np.zeros([10, new_length])
  apds_absolute_errors_cutoff = np.zeros([10, new_length])

  for apd in range(10):
    apds_percent_errors_cutoff[apd] = get_percentile_data(apds_percent_errors[apd], 10, 90)
    apds_absolute_errors_cutoff[apd] = get_percentile_data(apds_absolute_errors[apd], 10, 90)
  
  return pred_apds, correct_apds, apds_percent_errors_cutoff, apds_absolute_errors_cutoff

#Given numpy arrays of both the prediced traces and the actual traces, returns 
#numpy arrays of the 10 APDs taken from those traces
def get_predicted_apds(predicted_traces, actual_traces):  
  pred_widths_test = []
  raw_widths_test = []
  for pred_test,raw_test in zip(predicted_traces,actual_traces):
    w1_test, h1_test, l1_test = get_apds(pred_test, 2800)
    pred_widths_test.append(w1_test)
    w2_test, h2_test, l2_test = get_apds(raw_test, 2800)
    raw_widths_test.append(w2_test)
  pred_widths = np.asarray(pred_widths_test)
  raw_widths = np.asarray(raw_widths_test)
  return pred_widths, raw_widths


def boxplot_accuracy(apd_errors, color, positions, label):
    plt.boxplot(np.transpose(apd_errors),patch_artist=True,
            boxprops=dict(facecolor=color, color='k'),
            capprops=dict(color='k'),
            whiskerprops=dict(color=color),
            flierprops=dict(color=color, markeredgecolor=color),
            medianprops=dict(color='k'), widths = 0.1, positions = positions)

    plt.xticks([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], ['APD10', 'APD20', 'APD30', 'APD40', 'APD50', 'APD60', 'APD70', 'APD80', 'APD90', 'APD100'], rotation=90)
    plt.legend([label])
    
def boxplot_comparison2(errors_models, legends, ylabel):
    plt.rcParams["figure.figsize"] = (30,5)
    z = [0.9, 1.9, 2.9, 3.9, 4.9, 5.9, 6.9, 7.9, 8.9, 9.9]
    colors = ['r', 'g', 'b', 'black', 'purple', 'orange','m', 'y', 'c']
    for i in range(len(errors_models)):
        boxplot_accuracy(errors_models[i],  colors[i], [j + 0.1*i for j in z], legends[i])
    plt.ylabel(ylabel)
    plt.legend()

def boxplot_accuracy(apd_errors, color, positions, label):
    box = plt.boxplot(np.transpose(apd_errors), patch_artist=True,
                      boxprops=dict(facecolor=color, color='k'),
                      capprops=dict(color='k'),
                      whiskerprops=dict(color=color),
                      flierprops=dict(color=color, markeredgecolor=color),
                      medianprops=dict(color='k'), widths=0.1, positions=positions)

    plt.xticks([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], ['APD10', 'APD20', 'APD30', 'APD40', 'APD50', 'APD60', 'APD70', 'APD80', 'APD90', 'APD100'], rotation=90, fontsize=16)
    
    box_artist = box['boxes'][0]
    box_artist.set_facecolor(color)

    return box_artist, label

def boxplot_comparison2(errors_models, legends, ylabel):
    plt.rcParams["figure.figsize"] = (30, 7)
    z = [0.9, 1.9, 2.9, 3.9, 4.9, 5.9, 6.9, 7.9, 8.9, 9.9]
    colors = ['r', 'g', 'b', 'black', 'purple', 'orange', 'm', 'y', 'c']
    handles = []
    labels = []
    for i in range(len(errors_models)):
        box_artist, label = boxplot_accuracy(errors_models[i], colors[i], [j + 0.1*i for j in z], legends[i])
        handles.append(box_artist)
        labels.append(label)
    plt.ylabel(ylabel, fontsize=16)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=14)
    plt.legend(handles, labels, loc='upper left', fontsize=14)
# -*- coding: utf-8 -*-
"""utils.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1__83dVJ25N4ycknJ6TctQ7SBJJ5m4_f1
"""

# pip install McsPyDataTools

# pip install tsaug


import os
# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
import sys
from peak_finder import butter_bandpass_filter, get_peak_indices, find_poration_pulses
import pandas as pd
from scipy.signal import find_peaks,peak_widths
import os
import h5py as h5
import seaborn as sns
import glob
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.autograd import Variable
from torch.utils.data import Dataset, DataLoader
from scipy.signal import find_peaks,peak_widths
from sklearn.metrics import cohen_kappa_score,f1_score
from sklearn.model_selection import KFold, train_test_split
device = torch.device("cuda")
from sklearn.metrics import mean_squared_error
import tsaug
import peak_finder
import matplotlib.pyplot as plt

from scipy.signal import find_peaks,peak_widths
import random

#Takes in an numpy array of intracellular traces and returns the decay rates in a numpy array (% decay per second)
def get_decay_rates(intras):
    decay_rates = []

    for intra in intras:
        intra = np.asarray(moving_filter(intra, 20))

        max_value = max(intra[:2800])
        max_index = np.where(intra == max_value)[0][0]

        min_value = min(intra[max_index:3750])
        min_index = np.where(intra[max_index:] == min_value)[0][0] + max_index

        time_diff = (max_index - min_index) * .0002

        decay_rate = (max_value - min_value) / (time_diff)
        decay_rate = decay_rate * -100
        decay_rates.append(decay_rate)

    decay_rates = np.asarray(decay_rates)
    return decay_rates

#Given numpy arrays of both the prediced traces and the actual traces, returns 
#numpy arrays of the 10 APDs taken from those traces
def get_predicted_apds(predicted_traces, actual_traces):  
    pred_widths_test = []
    raw_widths_test = []
    for pred_test,raw_test in zip(predicted_traces,actual_traces):
        w1_test, h1_test, l1_test = get_apds(pred_test, 2800)
        pred_widths_test.append(w1_test)
        w2_test, h2_test, l2_test = get_apds(raw_test, 2800)
        raw_widths_test.append(w2_test)
    pred_widths = np.asarray(pred_widths_test)
    raw_widths = np.asarray(raw_widths_test)
    return pred_widths, raw_widths

#Takes in numpy arrays of the apd predictions and true apd values and returns
#the indices sorted by mse, and returns the mse values for the predicted values
from sklearn.metrics import mean_squared_error

def sort_by_error(predicted_apds, raw_apds):
  mse_apd = []
  for pred_val,raw_val in zip(predicted_apds, raw_apds):
    mse = mean_squared_error(raw_val, pred_val)
    mse_apd.append(mse)
  error_sort = np.argsort(mse_apd)
  return error_sort, mse_apd

#Takes in a 1 dimensional numpy array of a single trace (4000 points) and adds
#some random noise
def add_noise(intra_trace):
  intra_trace = intra_trace.reshape(sig_len_extra)
  std = np.std(intra_trace[0:500])
  noise = np.random.normal(0,std/2,len(intra_trace))
  noisy_trace = intra_trace + noise
  return noisy_trace

#Takes in a 1 dimensional numpy array and a window size, and applies a moving
#average filter to the data.
def moving_filter(data, window_size):
  i = 0
  moving_averages = []
  while i < len(data) - window_size + 1:
      this_window = data[i : i + window_size]

      window_average = sum(this_window) / window_size
      moving_averages.append(window_average)
      i += 1

  for i in range(window_size - 1):
      moving_averages.append(data[len(data) - (window_size - i)])

  return moving_averages

#Takes in a numpy array representing a single intracellular trace (4000 points), and
#two_peak_cutoff, which is a point after the action potential but before a potential second
#peak. Returns the apd widths, heights (y values of where they occur in action potential), and
#left values (x values of where apd starts in action potential)
def get_apds(intra_trace, two_peak_cutoff):
  intra_trace = np.asarray(moving_filter(intra_trace, 20))
  period = pd.Series(intra_trace)
  std = period.rolling(window=20).std()
  stdofstd = np.std(std[19:])
  stdMedian = np.median(std[19:])

  start_loc = np.where(std == np.max(std[19:1500]))[0][0]

  while (start_loc >= 1 and std[start_loc] > stdMedian + 0.2 * stdofstd):
    start_loc = start_loc - 1;

  w1 = []
  h1 = []
  l1 = []

  period = period[start_loc:]
  two_peak_cutoff = two_peak_cutoff - start_loc #try +

  locs = np.where(intra_trace[two_peak_cutoff:] > 0.85)[0] + two_peak_cutoff
  if len(locs) == 0:
  #for relheight in np.linspace(0.05,0.9,15):
    for relheight in np.linspace(0.1,1,10):
      w,h,l,r=peak_widths(period,find_peaks(period,distance=3500)[0],rel_height=relheight)
      w1.append(w[0])
      h1.append(h[0])
      l1.append(l[0] + start_loc)
      
  else:
    distance = locs[0] - start_loc - 500
    if distance < 1:
        print(distance)
        distance = 1
    for relheight in np.linspace(0.1,1,10):
      w,h,l,r=peak_widths(period,find_peaks(period[0:locs[0]-200], distance=distance)[0],rel_height=relheight)
      w1.append(w[0])
      h1.append(h[0])
      l1.append(l[0] + start_loc)
  return np.asarray(w1), np.asarray(h1), np.asarray(l1)

#Given numpy arrays of predicted apds and correct_apds, returns arrays containing
#the percent error and absolute error of each apd value

def get_apd_errors(predicted_apds, correct_apds):
    apds_percent_errors = np.zeros((10, len(predicted_apds)))
    apds_absolute_errors = np.zeros((10, len(predicted_apds)))
    for i in range(len(predicted_apds)):
        for apd in range(10):
            correct = correct_apds[i][apd]
            predicted = predicted_apds[i][apd]

            percent_error = abs(((predicted - correct) / correct) * 100)
            absolute_error = abs(predicted - correct) * .0002
            apds_percent_errors[apd][i] = abs(percent_error)
            apds_absolute_errors[apd][i] = abs(absolute_error)
    return apds_percent_errors, apds_absolute_errors

#Given a numpy array of data, a low percentile value, and a high percentile value,
# returns the data within the two given percentiles 
def get_percentile_data(data, low_percentile, high_percentile):
  cutoff_low = np.percentile(data, low_percentile)
  cutoff_high = np.percentile(data, high_percentile)
  within_range = data < cutoff_high
  new_data = data[within_range]
  within_range = new_data > cutoff_low
  new_data = new_data[within_range]
  return new_data

#Applies some random jitter to an array (used for violin plots)
def rand_jitter(arr):
    stdev = .05 * (np.mean(arr))
    return arr + np.random.randn(len(arr)) * stdev

#Given numpy arrays of the extra and intra testing data, and apd values, and a model, returns the
#predicted traces (intras), actual traces, and predicted apds
def test_model(extracellular_testing, intracellular_testing, apds_testing, model):
  sig_len_extra = len(extracellular_testing[0])
  sig_len_apd = len(apds_testing[0])

  extras_testing=extracellular_testing.reshape(-1,sig_len_extra,1) #needs to be a tensor for pytorch, so add a dimension to Nxlen_sigx1
  intras_testing=intracellular_testing.reshape(-1,sig_len_extra,1)
  apds_testing=apds_testing.reshape(-1,sig_len_apd, 1) #overall scale

  extras_noisy = []

  for extra in extras_testing:
    extra_noisy = add_noise(extra)
    extras_noisy.append(extra_noisy)

  extras_testing = np.asarray(extras_noisy)
  extras_testing=extras_testing.reshape(-1,sig_len_extra,1)

  batch_size = 4
  valid = ION_Dataset(extras_testing, apds_testing, intras_testing, mode='valid')

  x_test = torch.tensor(np.transpose(extras_testing,(0,2,1)), dtype=torch.float).cuda()
  test = torch.utils.data.TensorDataset(x_test)

  test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)

  model.eval()
  test_preds_apd=[]
  test_preds_trace=[]
  test_input=[]
  for i, x_batch in enumerate(test_loader):
      y_pred_apd = model(x_batch[0])[0].detach()
      y_pred_trace = model(x_batch[0])[1].detach()
      test_preds_apd.append(np.asarray(y_pred_apd.cpu().detach())[:,0,:])
      test_preds_trace.append(np.asarray(y_pred_trace.cpu().detach())[:,0,:])
      test_input.append(np.asarray(x_batch[0].cpu().detach())[:,0,:])

  test_preds_apd=np.concatenate(test_preds_apd)
  test_preds_apd = test_preds_apd * 2320.1992383    #scaling is constant, will not change
  test_preds_trace=np.concatenate(test_preds_trace)
  test_input=np.concatenate(test_input)

  intras_testing=intras_testing.reshape(-1,sig_len_extra)
  intras_testing = np.asarray(intras_testing)

  return test_preds_trace, intras_testing, test_preds_apd

def top_5_apds(apd_abs_all,list1,save_as, abs_ = True):
    
        apd_abs_all['phy']= apd_abs_all['physics'].apply(lambda x: 1 if x== True or x =='True' else 0)
        apd_abs_all['mae']= apd_abs_all['loss_'].apply(lambda x: 1 if x== 'mae'  else 0)
        apds = apd_abs_all.groupby('Model').mean()
        if abs_: 
            apds['avg'] = apds[['apd_abs_' + str(i) for i in range(10)]].mean(axis = 1)
            apds['std'] = apds[['apd_abs_' + str(i) for i in range(10)]].std(axis = 1)
        else: 
            apds['avg'] = apds[['apd_perc_' + str(i) for i in range(10)]].mean(axis = 1)
            apds['std'] = apds[['apd_perc_' + str(i) for i in range(10)]].std(axis = 1)
        apds = apds.reset_index()
       
        apds['loss'] = apds['mae'].apply(lambda x: 'mae' if x == 1 else 'mse')
        cols_to_name = ['Model','mae','kernel_size', 'ch_num', 'depth',  'phy','avg','std','w1','w2','w3']
        
        for j in apds.index:
            name= ''.join([i +'_'+  str(round(apds.loc[j,i],4)) + ' | ' for i in cols_to_name ])
            apds.loc[j,'name'] = name

        
        models_dic = {}
        names_ = []
        
        if abs_ :
            for j in list1:
                # print(j)
                models_dic[j] = np.array(apd_abs_all[apd_abs_all['Model'] ==j ][['apd_abs_' + str(i) for i in range(10)]]).T
                names_.append(apds.loc[j,'name'])
            
            boxplot_comparison2(list( models_dic.values()),
                                 names_, 'Absolute Error (s)')
            plt.savefig(save_as+'box_abs.png')
        else :
            for j in list1:

                models_dic[j] = np.array(apd_abs_all[apd_abs_all['Model'] ==j ][['apd_perc_' + str(i) for i in range(10)]]).T
                names_.append(apds.loc[j,'name'])
            
            boxplot_comparison2(list( models_dic.values()),
                                 names_, '% Error')
            plt.savefig(save_as+'box_perc.png')

def get_predicted_apds(predicted_traces, actual_traces):  
    pred_widths_test = []
    raw_widths_test = []
    for pred_test,raw_test in zip(predicted_traces,actual_traces):
        w1_test, h1_test, l1_test = get_apds(pred_test, 4800)
        pred_widths_test.append(w1_test)
        w2_test, h2_test, l2_test = get_apds(raw_test, 4800)
        raw_widths_test.append(w2_test)
    pred_widths = np.asarray(pred_widths_test)
    raw_widths = np.asarray(raw_widths_test)
    return pred_widths, raw_widths

def calculate_accuracy(intra_predictions, intra_correct):
    pred_apds, correct_apds = get_predicted_apds(intra_predictions[:,:,0], intra_correct[:,:])
    apds_percent_errors, apds_absolute_errors = get_apd_errors(pred_apds, correct_apds)

    new_length = len(get_percentile_data(apds_percent_errors[0], 10, 90))
    apds_percent_errors_cutoff = np.zeros([10, new_length])
    apds_absolute_errors_cutoff = np.zeros([10, new_length])

    for apd in range(10):
        apds_percent_errors_cutoff[apd] = get_percentile_data(apds_percent_errors[apd], 10, 90)
        apds_absolute_errors_cutoff[apd] = get_percentile_data(apds_absolute_errors[apd], 10, 90)

    # return pred_apds, correct_apds, apds_percent_errors_cutoff, apds_absolute_errors_cutoff
    return pred_apds, correct_apds, apds_percent_errors, apds_absolute_errors


#Takes in a numpy array representing a single intracellular trace (4000 points), and
#two_peak_cutoff, which is a point after the action potential but before a potential second
#peak. Returns the apd widths, heights (y values of where they occur in action potential), and
#left values (x values of where apd starts in action potential)
def get_apds(intra_trace, two_peak_cutoff):
  intra_trace = np.asarray(moving_filter(intra_trace, 20))
  period = pd.Series(intra_trace)
  std = period.rolling(window=20).std()
  stdofstd = np.std(std[19:])
  stdMedian = np.median(std[19:])

  start_loc = np.where(std == np.max(std[19:1500]))[0][0]

  while (start_loc >= 1 and std[start_loc] > stdMedian + 0.2 * stdofstd):
    start_loc = start_loc - 1;

  w1 = []
  h1 = []
  l1 = []

  period = period[start_loc:]
  two_peak_cutoff = two_peak_cutoff - start_loc #try +

  locs = np.where(intra_trace[two_peak_cutoff:] > 0.85)[0] + two_peak_cutoff
  if len(locs) == 0:
  #for relheight in np.linspace(0.05,0.9,15):
    for relheight in np.linspace(0.1,1,10):
      w,h,l,r=peak_widths(period,find_peaks(period,distance=3500)[0],rel_height=relheight)
      w1.append(w[0])
      h1.append(h[0])
      l1.append(l[0] + start_loc)
      
  else:
    distance = locs[0] - start_loc - 500
    if distance < 1:
        print(distance)
        distance = 1
    for relheight in np.linspace(0.1,1,10):
      w,h,l,r=peak_widths(period,find_peaks(period[0:locs[0]-200], distance=distance)[0],rel_height=relheight)
      w1.append(w[0])
      h1.append(h[0])
      l1.append(l[0] + start_loc)
  return np.asarray(w1), np.asarray(h1), np.asarray(l1)

#Given numpy arrays of predicted apds and correct_apds, returns arrays containing
#the percent error and absolute error of each apd value

#Given a numpy array of data, a low percentile value, and a high percentile value,
# returns the data within the two given percentiles 
def get_percentile_data(data, low_percentile, high_percentile):
  cutoff_low = np.percentile(data, low_percentile)
  cutoff_high = np.percentile(data, high_percentile)
  within_range = data < cutoff_high
  new_data = data[within_range]
  within_range = new_data > cutoff_low
  new_data = new_data[within_range]
  return new_data

def boxplot_accuracy(apd_errors, color, positions):
  plt.boxplot(np.transpose(apd_errors),patch_artist=True,
            boxprops=dict(facecolor=color, color='k'),
            capprops=dict(color='k'),
            whiskerprops=dict(color=color),
            flierprops=dict(color=color, markeredgecolor=color),
            medianprops=dict(color='k'), widths = 0.1, positions = positions)

  plt.xticks([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], ['APD10', 'APD20', 'APD30', 'APD40', 'APD50', 'APD60', 'APD70', 'APD80', 'APD90', 'APD100'], rotation=90)

              
def boxplot_accuracy(apd_errors, color, positions):
  plt.boxplot(np.transpose(apd_errors),patch_artist=True,
            boxprops=dict(facecolor=color, color='k'),
            capprops=dict(color='k'),
            whiskerprops=dict(color=color),
            flierprops=dict(color=color, markeredgecolor=color),
            medianprops=dict(color='k'), widths = 0.1, positions = positions)

  plt.xticks([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], ['APD10', 'APD20', 'APD30', 'APD40', 'APD50', 'APD60', 'APD70', 'APD80', 'APD90', 'APD100'], rotation=90)

#Given the errors from 2 models, plots them together to compare the percent/absolute
#errors.
def boxplot_comparison(errors_model_1, errors_model_2, ylabel):
  plt.rcParams["figure.figsize"] = (12,4)
  boxplot_accuracy(errors_model_1, 'b', [0.9, 1.9, 2.9, 3.9, 4.9, 5.9, 6.9, 7.9, 8.9, 9.9])
  boxplot_accuracy(errors_model_2, 'r', [1.1, 2.1, 3.1, 4.1, 5.1, 6.1, 7.1, 8.1, 9.1, 10.1])
  plt.ylabel(ylabel)

def boxplot_comparison2(errors_model_1, errors_model_2,errors_model_3, ylabel):
  plt.rcParams["figure.figsize"] = (12,4)
  boxplot_accuracy(errors_model_1, 'b', [0.9, 1.9, 2.9, 3.9, 4.9, 5.9, 6.9, 7.9, 8.9, 9.9])
  boxplot_accuracy(errors_model_2, 'r', [1.1, 2.1, 3.1, 4.1, 5.1, 6.1, 7.1, 8.1, 9.1, 10.1])
  boxplot_accuracy(errors_model_3, 'g', [1.3, 2.3, 3.3, 4.3, 5.3, 6.3, 7.3, 8.3, 9.3, 10.3])
  plt.ylabel(ylabel)

#Given the intracellular traces, predicted traces, extracellular traces, and the sorted indices,
#plots the 5 best predictions from the dataset along with their extracellular counterparts.
def plot_best_predictions(intras_test, pred_intras_test, extras_test, test_error_sort):
  f,ax=plt.subplots(2,5,figsize=(16,3))

  color = (1.00,0.41,0.16)
  for ranking in range(5):
    idx = test_error_sort[ranking]
    ax[0, ranking].plot(intras_test[idx], color='b', linewidth = 4)
    ax[0, ranking].plot(pred_intras_test[idx], color=color)
    #plt.legend(['Actual', 'Predicted'])

    ax[1, ranking].plot(np.asarray(extras_test)[idx], color='b', linewidth = 4)
    ax[1, ranking].set_ylim(-0.5, 0.5)
#Given the intracellular traces, predicted traces, extracellular traces, and the sorted indices,
#plots the 5 worst predictions from the dataset along with their extracellular counterparts.
def plot_worst_predictions(intras_test, pred_intras_test, extras_test, test_error_sort):
  f,ax=plt.subplots(2,5,figsize=(16,3))

  color = (1.00,0.41,0.16)
  for i in range(5):
    idx = test_error_sort[len(intras_test) - i - 1]
    ax[0, i].plot(intras_test[idx], color='b', linewidth = 4)
    ax[0, i].plot(pred_intras_test[idx], color=color)
    #plt.legend(['Actual', 'Predicted'])

    ax[1, i].plot(np.asarray(extras_test)[idx], color='b', linewidth = 4)
    ax[1, i].set_ylim(-0.5, 0.5)
    
def calculate_accuracy(intra_predictions, intra_correct):
  pred_apds, correct_apds = get_predicted_apds(intra_predictions[:,:,0], intra_correct[:,:])
  apds_percent_errors, apds_absolute_errors = get_apd_errors(pred_apds, correct_apds)

  new_length = len(get_percentile_data(apds_percent_errors[0], 10, 90))
  apds_percent_errors_cutoff = np.zeros([10, new_length])
  apds_absolute_errors_cutoff = np.zeros([10, new_length])

  for apd in range(10):
    apds_percent_errors_cutoff[apd] = get_percentile_data(apds_percent_errors[apd], 10, 90)
    apds_absolute_errors_cutoff[apd] = get_percentile_data(apds_absolute_errors[apd], 10, 90)
  
  return pred_apds, correct_apds, apds_percent_errors_cutoff, apds_absolute_errors_cutoff

#Given numpy arrays of both the prediced traces and the actual traces, returns 
#numpy arrays of the 10 APDs taken from those traces
def get_predicted_apds(predicted_traces, actual_traces):  
  pred_widths_test = []
  raw_widths_test = []
  for pred_test,raw_test in zip(predicted_traces,actual_traces):
    w1_test, h1_test, l1_test = get_apds(pred_test, 2800)
    pred_widths_test.append(w1_test)
    w2_test, h2_test, l2_test = get_apds(raw_test, 2800)
    raw_widths_test.append(w2_test)
  pred_widths = np.asarray(pred_widths_test)
  raw_widths = np.asarray(raw_widths_test)
  return pred_widths, raw_widths


def boxplot_accuracy(apd_errors, color, positions, label):
    plt.boxplot(np.transpose(apd_errors),patch_artist=True,
            boxprops=dict(facecolor=color, color='k'),
            capprops=dict(color='k'),
            whiskerprops=dict(color=color),
            flierprops=dict(color=color, markeredgecolor=color),
            medianprops=dict(color='k'), widths = 0.1, positions = positions)

    plt.xticks([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], ['APD10', 'APD20', 'APD30', 'APD40', 'APD50', 'APD60', 'APD70', 'APD80', 'APD90', 'APD100'], rotation=90)
    plt.legend([label])
    
def boxplot_comparison2(errors_models, legends, ylabel):
    plt.rcParams["figure.figsize"] = (30,5)
    z = [0.9, 1.9, 2.9, 3.9, 4.9, 5.9, 6.9, 7.9, 8.9, 9.9]
    colors = ['r', 'g', 'b', 'black', 'purple', 'orange','m', 'y', 'c']
    for i in range(len(errors_models)):
        boxplot_accuracy(errors_models[i],  colors[i], [j + 0.1*i for j in z], legends[i])
    plt.ylabel(ylabel)
    plt.legend()

def boxplot_accuracy(apd_errors, color, positions, label):
    box = plt.boxplot(np.transpose(apd_errors), patch_artist=True,
                      boxprops=dict(facecolor=color, color='k'),
                      capprops=dict(color='k'),
                      whiskerprops=dict(color=color),
                      flierprops=dict(color=color, markeredgecolor=color),
                      medianprops=dict(color='k'), widths=0.1, positions=positions)

    plt.xticks([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], ['APD10', 'APD20', 'APD30', 'APD40', 'APD50', 'APD60', 'APD70', 'APD80', 'APD90', 'APD100'], rotation=90, fontsize=16)
    
    box_artist = box['boxes'][0]
    box_artist.set_facecolor(color)

    return box_artist, label

def boxplot_comparison2(errors_models, legends, ylabel):
    plt.rcParams["figure.figsize"] = (30, 7)
    z = [0.9, 1.9, 2.9, 3.9, 4.9, 5.9, 6.9, 7.9, 8.9, 9.9]
    colors = ['r', 'g', 'b', 'black', 'purple', 'orange', 'm', 'y', 'c']
    handles = []
    labels = []
    for i in range(len(errors_models)):
        box_artist, label = boxplot_accuracy(errors_models[i], colors[i], [j + 0.1*i for j in z], legends[i])
        handles.append(box_artist)
        labels.append(label)
    plt.ylabel(ylabel, fontsize=16)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=14)
    plt.legend(handles, labels, loc='upper left', fontsize=14)

def compute_average_dtw(actual, predicted):
    num_data_points = actual.shape[0]
    dtw_distances = []

    for i in range(num_data_points):
        distance, _ = fastdtw(actual[i], predicted[i])
        dtw_distances.append(distance)

    avg_dtw_distance = np.mean(dtw_distances)
    return avg_dtw_distance

def error_report(pred, real, apd_pred, apd_real):
    mses =[]
    maes= []
    dtws = []
    apd_as= []
    apd_ps=[]
    for i in range(len(real)):
        mse_ = mean_squared_error(real[i], pred[i], squared=True)
        mae_ = mean_absolute_error(real[i], pred[i])
        dtw_ = compute_average_dtw(real[i].reshape(1,-1), pred[i].reshape(1,-1))
        percent_error = abs(((apd_pred[i] -  apd_real[i]) /  apd_real[i]) * 100)
        absolute_error = abs(apd_pred[i] -  apd_real[i]) * .0002
        mses.append(mse_)
        maes.append(mae_)
        dtws.append(dtw_)
        apd_as.append(absolute_error)
        apd_ps.append(percent_error)
    apd_as = np.array(apd_as)
    apd_ps = np.array(apd_ps)
    
    mse_m = np.mean(mses)
    mse_s = np.std(mses)
    mae_m = np.mean(maes)
    mae_s = np.std(maes)
    dtws_m = np.mean(dtws)
    dtws_s = np.std(dtws)
    
    
    apd_10_am = apd_as.mean(axis = 0)
    apd_10_as = apd_as.std(axis = 0)
    apd_10_pm = apd_ps.mean(axis = 0)
    apd_10_ps = apd_ps.std(axis = 0)
    
    apds_allas_mean =  apd_as.mean()
    apds_allas_std =  apd_as.std()
    apds_allap_mean =  apd_ps.mean()
    apds_allap_std =  apd_ps.std()

    return mse_m ,mse_s,mae_m,mae_s ,dtws_m,dtws_s, apd_10_am ,apd_10_as, apd_10_pm,apd_10_ps ,  apds_allas_mean,apds_allas_std, apds_allap_mean, apds_allap_std 




